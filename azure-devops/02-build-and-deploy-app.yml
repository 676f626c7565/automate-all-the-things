name: application-build

trigger:
  branches:
    include:
      - main  # Include the branches where you want the trigger to apply
  paths:
    include:
      - my-app/*

variables:
  - group: aws-keys

pool:
  vmImage: 'ubuntu-latest' 
  # If you are using a self-hosted agent, comment out the previous line and uncomment the following three
  # name: <agent-pool-name> # Insert here the name of the agent pool you created
  # demands:
  #   - agent.name -equals <agent-name> # Insert here the name of the agent you created

jobs:
- job: BuildJob
  displayName: 'Build Job'
  steps:
  - task: DockerInstaller@0
    displayName: Install Docker
    inputs:
      dockerVersion: '17.09.0-ce'
    
  - task: Docker@2
    displayName: Build And Push Image
    inputs:
      containerRegistry: 'dockerhub'
      repository: 'AATT_DOCKERHUB_USERNAME/AATT_APP_NAME' # This value was modified by the initial-setup python script
      command: 'buildAndPush'
      Dockerfile: 'my-app/Dockerfile'

- job: DeployJob
  displayName: 'Deploy Job'
  dependsOn: BuildJob
  steps:

  # In this case it's necessary to specify the checkout with the persistCredential options set to true. This will enable us to push the changes to the repo.
  - checkout: self
    persistCredentials: true

  - script: |
      sed 's/tag:.*/tag: $(Build.BuildId)/g' helm/my-app/values.yaml > helm/my-app/values.temp
      mv helm/my-app/values.temp helm/my-app/values.yaml
    displayName: Update Tag In values.yaml

  - script: |
      git config --global user.email "AzureDevOps@Build&DeployAppPipeline.com"
      git config --global user.name "Azure DevOps - Build & Deploy App Pipeline"
    displayName: 'Configure Git user'

  - script: |
      git checkout -b main
      git add helm/my-app/values.yaml 
      git commit -m "App version tag updated to $(Build.BuildId) by Azure DevOps"
      git push --set-upstream origin main
    displayName: 'Push changes to GitHub'

  - script: | # This value was modified by the initial-setup python script
      mkdir ~/.aws
      echo -e "[default]\naws_access_key_id = $(aws_access_key_id)\naws_secret_access_key = $(aws_secret_access_key)" > ~/.aws/credentials
      echo -e "[default]\nregion = AATT_AWS_REGION"> ~/.aws/config 
    displayName: 'Configure AWS Profile'
    
  - script: |
      echo "false" > is-active.txt
      while [ "$(cat is-active.txt)" != "true" ]; do
          sleep 5
          aws elbv2 describe-load-balancers > lb-status.json
          json_data=$(cat lb-status.json)
          state_code=$(echo "$json_data" | jq -r '.LoadBalancers[1].State.Code')
          if [[ "$state_code" == "active" ]]; then
            echo "true" > is-active.txt
          else
            echo "Load balancer is not ready yet..."
          fi
      done
    displayName: 'Wait for Load Balancer Status'

  - task: AWSCLI@1
    displayName: 'Update KubeConfig'
    inputs:
      awsCredentials: 'aws'
      regionName: 'AATT_AWS_REGION' # This value was modified by the initial-setup python script
      awsCommand: 'eks'
      awsSubCommand: 'update-kubeconfig'
      awsArguments: '--name AATT_APP_NAME-cluster --region AATT_AWS_REGION' # This value was modified by the initial-setup python script

  - script: | # This value was modified by the initial-setup python script
      kubectl get ingress -n AATT_APP_NAME $(kubectl get ingress -n AATT_APP_NAME | awk 'NR>1{print $1}') -o=jsonpath="{'http://'}{.status.loadBalancer.ingress[].hostname}{'\n'}" > my-app-url.txt
    displayName: 'Save URL'

  - task: PublishBuildArtifacts@1
    displayName: 'Export URL'
    inputs:
      PathtoPublish: 'my-app-url.txt'
      ArtifactName: 'URL'
      publishLocation: 'Container'